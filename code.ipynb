{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|NetId|Approach|\n",
    "|-|-||\n",
    "|Zhengchuan Liang|zlian064|Perceptron|\n",
    "|Diana Men|lmen004|Logistic Regression|\n",
    "|Xing Gao|xgao058|Naive Bayes|\n",
    "|Yuwei Zhang|yzhan995|ID3|\n",
    "|Shixun Wu|swu264|Cart|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries should be imported first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# shared code\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# shared code\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our dataset into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `X` (a matrix of the features values) and `y` (a vector of the labels) from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "y: pd.Series = df['NObeyesdad'].str.startswith('Obesity').astype(int)\n",
    "del df['NObeyesdad']\n",
    "del df['Height']\n",
    "del df['Weight']\n",
    "X: pd.DataFrame = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, precision, recall and f1 are our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "def evaluate(y_actual, y_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    print('accuracy:', accuracy_score(y_actual, y_pred))\n",
    "    print('precision:', precision_score(y_actual, y_pred))\n",
    "    print('recall:', recall_score(y_actual, y_pred))\n",
    "    print('f1:', f1_score(y_actual, y_pred))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since perceptron only accepts numeric values, I need to map categorical values to numeric ones.\n",
    "For `MTRANS` feature, I believe the number reflects the amount of exercise.\n",
    "\n",
    "Then normalize all features using min-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_perceptron(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    freq_map = {'no': 0, 'Sometimes': 1,\n",
    "                'Frequently': 2, 'Always': 3}\n",
    "\n",
    "    yes_no_map = {'yes': 1, 'no': 0}\n",
    "\n",
    "    X = X.copy()\n",
    "    X['Gender'] = X['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    X['family_history_with_overweight'] = X['family_history_with_overweight'].map(\n",
    "        yes_no_map)\n",
    "    X['FAVC'] = X['FAVC'].map(yes_no_map)\n",
    "    X['CAEC'] = X['CAEC'].map(freq_map)\n",
    "    X['SMOKE'] = X['SMOKE'].map(yes_no_map)\n",
    "    X['SCC'] = X['SCC'].map(yes_no_map)\n",
    "    X['CALC'] = X['CALC'].map(freq_map)\n",
    "    X['MTRANS'] = X['MTRANS'].map(\n",
    "        {'Automobile': 0, 'Motorbike': 0, 'Public_Transportation': 0, 'Walking': 1, 'Bike': 1})\n",
    "\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function of my perceptron is a step funcion:\n",
    "$$\n",
    "F(net)=1\\text{ if } net>0,0\\text{ otherwise}\\\\\n",
    "net=\\sum_{i=0}^nw_ix_i=\\textbf{w}^T\\textbf{x}\n",
    "$$\n",
    "\n",
    "For a single sample $(\\textbf{x},y)$ where $\\textbf{x}\\in\\mathbb{R}^n$ and $y\\in\\mathbb{R}$, the loss function is:\n",
    "$$\n",
    "L(\\textbf{w})=(F(net)-y)\\textbf{w}^T\\textbf{x}\n",
    "$$\n",
    "\n",
    "And the gradient is:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i}L(\\textbf{w})=(F(net)-y)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla L(\\textbf{w})=(F(net)-y)\\textbf{x}\n",
    "$$\n",
    "\n",
    "In my implementation, I use **batch gradient** for all samples instead.\n",
    "\n",
    "Given features matrix $\\textbf{X}\\in\\mathbb{R}^{m\\times n}$ and label vector $\\textbf{y}\\in\\mathbb{R}^{m}$, the steps to train a perceptron are as follows:\n",
    "\n",
    "Initialize weights $\\textbf{w}$ with a random vector.\n",
    "\n",
    "Predict the labels $\\textbf{y}_{pred}$ using the perceptron.\n",
    "\n",
    "The loss function is:\n",
    "$$\n",
    "L(\\textbf{w})=(\\textbf{y}_{pred}-\\textbf{y})^T\\textbf{X}\\textbf{w}\n",
    "$$\n",
    "\n",
    "The batch gradient is:\n",
    "$$\n",
    "\\nabla L(\\textbf{w})^T=\\frac{1}{m}(\\textbf{y}_{pred}-\\textbf{y})^T\\textbf{X}\n",
    "$$\n",
    "\n",
    "Update the gradient:\n",
    "$$\n",
    "\\textbf{w}\\leftarrow \\textbf{w}-\\eta\\nabla L(\\textbf{w})\n",
    "$$\n",
    "\n",
    "Repeat the above steps for 200 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" add a column of ones to the matrix\n",
    "    \"\"\"\n",
    "    X0 = np.ones((X.shape[0], 1))\n",
    "    return np.hstack((X0, X))\n",
    "\n",
    "\n",
    "class MyPerceptron:\n",
    "    def __init__(self, lr: float) -> None:\n",
    "        self.lr: float = lr  # learning rate\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, seed: int = 0) -> None:\n",
    "        assert X.ndim == 2\n",
    "        assert y.ndim == 1\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X = add_ones_column(X)\n",
    "        m = X.shape[0]  # number of samples\n",
    "        n = X.shape[1]  # number of features\n",
    "\n",
    "        # weights\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        init_W = rng.random(n)\n",
    "        init_W /= np.linalg.norm(init_W)  # initial weights are a random unit vector\n",
    "        self.W: np.ndarray = init_W\n",
    "\n",
    "        for epoch in range(200):\n",
    "            # predictions for all samples. vector of size m\n",
    "            y_pred = np.where(np.dot(X, self.W) > 0, 1, 0)\n",
    "\n",
    "            # (average) gradient for all samples. vector of size n\n",
    "            gradient = (y_pred - y) @ X / m\n",
    "\n",
    "            # loss for all samples\n",
    "            loss = (y_pred - y) @ (X @ self.W)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= self.lr * gradient\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = add_ones_column(X)\n",
    "        return np.where(np.dot(X, self.W) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the randomness of train_test_split, I split the dataset with 50 different random_states. For each split I train the off-the-shelf implementation and mine using the training set and predict the labels for the test set. I concatenate the $y_{actual}$ and $y_{pred}$ of each split and use them to compute the metrics.\n",
    "\n",
    "My implementation performs better than off-the-shelf on f1 score. This may result from batch gradient descent, whereas the off-the-shelf uses stochastic gradient descent.\n",
    "\n",
    "I also compute the cosine similarity between the weights of off-the-shelf perceptron and mine. The result shows that the weights of my implementation are very similar to that of off-the-shelf. It is much higher than the average similarity of two random 15-dimensional vectors (around 0.756). This demonstrates that my implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    \"\"\" cosine similarity\n",
    "    \"\"\"\n",
    "    return x @ y / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "\n",
    "def run_perceptron(X: pd.DataFrame, y: pd.Series):\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = preprocessing_perceptron(X)\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    y_test_all = np.empty(shape=(0,))\n",
    "    y_pred_shelf_all = np.empty(shape=(0,))\n",
    "    y_pred_my_all = np.empty(shape=(0,))\n",
    "    cosine_all = np.empty(shape=(0,))\n",
    "    for r in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.1, random_state=r, stratify=y)\n",
    "        y_test_all = np.append(y_test_all, y_test)\n",
    "\n",
    "        # off-the-shelf library\n",
    "        shelf = Perceptron(tol=1e-3, random_state=r)\n",
    "        shelf.fit(X_train, y_train)  # train using training set\n",
    "        y_pred_shelf = shelf.predict(X_test)  # predict for test set\n",
    "        y_pred_shelf_all = np.append(y_pred_shelf_all, y_pred_shelf)\n",
    "        W_shelf = np.append(shelf.intercept_, shelf.coef_.flatten())  # weights of library implementation\n",
    "\n",
    "        # my implementation\n",
    "        my = MyPerceptron(1)\n",
    "        my.fit(X_train, y_train, seed=r)  # train using training set\n",
    "        y_pred_my = my.predict(X_test)  # predict for test set\n",
    "        y_pred_my_all = np.append(y_pred_my_all, y_pred_my)\n",
    "        W_my = my.W  # weights of my implementation\n",
    "\n",
    "        # cosine similarity\n",
    "        cosine = cosine_sim(W_shelf, W_my)\n",
    "        cosine_all = np.append(cosine_all, cosine)\n",
    "\n",
    "    print('[+] off-the-shelf implementation:')\n",
    "    evaluate(y_test_all, y_pred_shelf_all)\n",
    "    print()\n",
    "\n",
    "    print('[+] my implementation:')\n",
    "    evaluate(y_test_all, y_pred_my_all)\n",
    "    print()\n",
    "\n",
    "    # average cosine similarity\n",
    "    # between the weights of library and my implementation\n",
    "    print(\"[+] average cosine similarity:\", cosine_all.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] off-the-shelf implementation:\n",
      "accuracy: 0.6788679245283019\n",
      "precision: 0.6497797356828194\n",
      "recall: 0.6622448979591836\n",
      "f1: 0.6559531028906408\n",
      "confusion matrix:\n",
      "[[3951 1749]\n",
      " [1655 3245]]\n",
      "\n",
      "[+] my implementation:\n",
      "accuracy: 0.6800943396226415\n",
      "precision: 0.5956880152187698\n",
      "recall: 0.9585714285714285\n",
      "f1: 0.7347673054360578\n",
      "confusion matrix:\n",
      "[[2512 3188]\n",
      " [ 203 4697]]\n",
      "\n",
      "[+] average cosine similarity: 0.9453590450484168\n"
     ]
    }
   ],
   "source": [
    "run_perceptron(X.copy(), y.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression(X: pd.DataFrame, y: pd.Series):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(X.copy(), y.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocessing\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[0;32m      2\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOneDrive - email.ucr.edu\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcs235\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mObesityDataSet_raw_and_data_sinthetic.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNObeyesdad\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObesity\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocessing() -> pd.DataFrame:\n",
    "    df = pd.read_csv('D:\\OneDrive - email.ucr.edu\\cs235\\data\\ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "    df['Label'] = df['NObeyesdad'].str.startswith('Obesity').astype(int)\n",
    "    del df['NObeyesdad']\n",
    "    del df['Height']\n",
    "    del df['Weight']\n",
    "    #\n",
    "    freq_map = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n",
    "\n",
    "    yes_no_map = {'yes': 1, 'no': 0}\n",
    "    #\n",
    "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    df['family_history_with_overweight'] = df['family_history_with_overweight'].map(yes_no_map)\n",
    "    df['FAVC'] = df['FAVC'].map(yes_no_map)\n",
    "    df['CAEC'] = df['CAEC'].map(freq_map)\n",
    "    df['SMOKE'] = df['SMOKE'].map(yes_no_map)\n",
    "    df['SCC'] = df['SCC'].map(yes_no_map)\n",
    "    df['CALC'] = df['CALC'].map(freq_map)\n",
    "    df['MTRANS'] = df['MTRANS'].map(\n",
    "        {'Automobile': 0, 'Motorbike': 0, 'Public_Transportation': 0, 'Walking': 1, 'Bike': 1})\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### My implemention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self):\n",
    "        self.classes = ['1', '0']\n",
    "        print('')\n",
    "\n",
    "    def gauss(self, test, mean, std):\n",
    "        t1 = (test - mean) * (test - mean)\n",
    "        t2 = std * std\n",
    "        res = np.exp(-t1/(t2*2)) / np.sqrt(2*t2*np.pi)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit2 (self, x_train: np.ndarray, y_train: np.ndarray):\n",
    "        self.normalData = []\n",
    "        self.obesityData = []\n",
    "        self.norMean = []\n",
    "        self.norStd = []\n",
    "        self.obeMean = []\n",
    "        self.obeStd = []\n",
    "\n",
    "\n",
    "        for i in range(y_train.shape[0]):\n",
    "            if y_train[i] == 0:\n",
    "                self.normalData.append(x_train[i])\n",
    "            else:\n",
    "                self.obesityData.append(x_train[i])\n",
    "\n",
    "\n",
    "        self.normalData = np.array(self.normalData)\n",
    "        self.obesityData = np.array(self.obesityData)\n",
    "\n",
    "        self.norMean = self.normalData.mean(axis=0)\n",
    "        self.norStd = self.normalData.std(axis=0)\n",
    "        self.obeMean = self.obesityData.mean(axis=0)\n",
    "        self.obeStd = self.obesityData.std(axis=0)\n",
    "        self.norPY = (y_train.shape[0]- np.sum(y_train)) / y_train.shape[0]\n",
    "        self.obePY = np.sum(y_train)/ y_train.shape[0]\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self,x_test: np.ndarray):\n",
    "        y_prob = []\n",
    "\n",
    "        for i in range(x_test.shape[0]):\n",
    "            curNor = self.norPY\n",
    "            curObe = self.obePY\n",
    "            for j in range(x_test.shape[1]):\n",
    "                curNor *= self.gauss(x_test[i,j],self.norMean[j],self.norStd[j])\n",
    "                curObe *= self.gauss(x_test[i,j],self.obeMean[j],self.obeStd[j])\n",
    "            y_prob.append(int(curObe > curNor))\n",
    "\n",
    "        return np.array(y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = preprocessing()\n",
    "    X = df.drop(columns=['Label'])\n",
    "    y = df['Label']\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    X.astype(np.int32)\n",
    "    y.astype(np.int32)\n",
    "\n",
    "    myPredAll = np.empty(shape=(0,))\n",
    "    sklPredAll = np.empty(shape=(0,))\n",
    "    y_test_all = np.empty(shape=(0,))\n",
    "\n",
    "    for i in range(50):\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i, stratify=y)\n",
    "        y_test_all = np.append(y_test_all, y_test)\n",
    "\n",
    "        nb = NaiveBayes()\n",
    "        nb.fit2(X_train,y_train)\n",
    "        myPred = nb.predict(X_test)\n",
    "        myPredAll = np.append(myPredAll,myPred)\n",
    "\n",
    "        clf = GaussianNB()\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        sklPred = clf.predict(X_test)\n",
    "        sklPredAll = np.append(sklPredAll, sklPred)\n",
    "\n",
    "    print('[+] off-the-shelf implementation:')\n",
    "    evaluate(y_test_all, sklPredAll)\n",
    "    print()\n",
    "\n",
    "    print('[+] my implementation:')\n",
    "    evaluate(y_test_all, myPredAll)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_naive_bayes(X.copy(), y.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[+] off-the-shelf implementation:\n",
    "accuracy: 0.7536908517350158\n",
    "precision: 0.6609783845278726\n",
    "recall: 0.955068493150685\n",
    "f1: 0.7812640071716719\n",
    "confusion matrix:\n",
    "[[ 9948  7152]\n",
    " [  656 13944]]\n",
    "\n",
    "[+] my implementation:\n",
    "accuracy: 0.7350788643533123\n",
    "precision: 0.6640567135752831\n",
    "recall: 0.8597260273972602\n",
    "f1: 0.7493283983045789\n",
    "confusion matrix:\n",
    "[[10750  6350]\n",
    " [ 2048 12552]]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_id3(X: pd.DataFrame, y: pd.Series):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id3(X.copy(), y.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cart(X: pd.DataFrame, y: pd.Series):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cart(X.copy(), y.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}