{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9678a1ad",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c372f",
   "metadata": {},
   "source": [
    "## Student Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa4297",
   "metadata": {},
   "source": [
    "|Name|NetId|\n",
    "|-|-|\n",
    "|Zhengchuan Liang|zlian064|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d3db9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844ba18",
   "metadata": {},
   "source": [
    "Some libraries should be imported first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9179238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22957553",
   "metadata": {},
   "source": [
    "Load our dataset into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6580524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f9c95",
   "metadata": {},
   "source": [
    "Get `X` (a matrix of the features values) and `y` (a vector of the labels) from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b29f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "y: pd.Series = df['NObeyesdad'].str.startswith('Obesity').astype(int)\n",
    "del df['NObeyesdad']\n",
    "del df['Height']\n",
    "del df['Weight']\n",
    "X: pd.DataFrame = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a673e17",
   "metadata": {},
   "source": [
    "Accuracy, precision, recall and f1 are our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730be25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "def evaluate(y_actual, y_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    print('accuracy:', accuracy_score(y_actual, y_pred))\n",
    "    print('precision:', precision_score(y_actual, y_pred))\n",
    "    print('recall:', recall_score(y_actual, y_pred))\n",
    "    print('f1:', f1_score(y_actual, y_pred))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c3fca",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bfad9",
   "metadata": {},
   "source": [
    "Since perceptron only accepts numeric values, I need to map categorical values to numeric ones.\n",
    "For `MTRANS` feature, I believe the number reflects the amount of exercise.\n",
    "\n",
    "Then normalize all features using min-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af42f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_perceptron(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    freq_map = {'no': 0, 'Sometimes': 1,\n",
    "                'Frequently': 2, 'Always': 3}\n",
    "\n",
    "    yes_no_map = {'yes': 1, 'no': 0}\n",
    "\n",
    "    X = X.copy()\n",
    "    X['Gender'] = X['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    X['family_history_with_overweight'] = X['family_history_with_overweight'].map(\n",
    "        yes_no_map)\n",
    "    X['FAVC'] = X['FAVC'].map(yes_no_map)\n",
    "    X['CAEC'] = X['CAEC'].map(freq_map)\n",
    "    X['SMOKE'] = X['SMOKE'].map(yes_no_map)\n",
    "    X['SCC'] = X['SCC'].map(yes_no_map)\n",
    "    X['CALC'] = X['CALC'].map(freq_map)\n",
    "    X['MTRANS'] = X['MTRANS'].map(\n",
    "        {'Automobile': 0, 'Motorbike': 0, 'Public_Transportation': 0, 'Walking': 1, 'Bike': 1})\n",
    "\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe971",
   "metadata": {},
   "source": [
    "## My Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bc601",
   "metadata": {},
   "source": [
    "The activation function of my perceptron is a step funcion:\n",
    "\n",
    "\\begin{equation}\n",
    "F(net)=\n",
    "\\begin{cases}\n",
    "1 & net>0 \\\\\n",
    "0 & net\\le 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$$\n",
    "net=\\sum_{i=0}^nw_ix_i=\\mathbf{w}^T\\mathbf{x}\\text{, where }x_0=1\n",
    "$$\n",
    "\n",
    "Therefore, the perceptron is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x})=F(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Then we want to minimize the sum of distance between each misclassified sample to the decision boundary.\n",
    "\n",
    "For a single sample $(\\mathbf{x},y)$ where $\\mathbf{x}\\in\\mathbb{R}^n$ and $y\\in\\mathbb{R}$,\n",
    "the distance is:\n",
    "\n",
    "$$\n",
    "d\n",
    "=\\frac{|\\sum_{i=0}^nw_ix_i|}{\\sqrt{\\sum_{i=1}^nw_i^2}}\n",
    "=\\frac{(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}}{||\\mathbf{w}||}\n",
    "$$\n",
    "\n",
    "We can ignore $||\\mathbf{w}||$ (can be proved), then the loss function is:\n",
    "$$\n",
    "L(\\mathbf{w})=(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "\n",
    "And the gradient is:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i}L(\\mathbf{w})=(P(\\mathbf{x})-y)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla L(\\mathbf{w})=(P(\\mathbf{x})-y)\\mathbf{x}\n",
    "$$\n",
    "\n",
    "In my implementation, I use **batch gradient** for all samples instead.\n",
    "\n",
    "Given features matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ and label vector $\\mathbf{y}\\in\\mathbb{R}^{m}$, the steps to train a perceptron are as follows:\n",
    "\n",
    "Initialize weights $\\mathbf{w}$ with a random vector.\n",
    "\n",
    "Predict the labels $\\mathbf{y}_{pred}$ using the perceptron.\n",
    "\n",
    "The sum of loss is:\n",
    "$$\n",
    "L(\\mathbf{w})\n",
    "=\\sum_{(\\mathbf{x},y)}(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}\n",
    "=(\\mathbf{y}_{pred}-\\mathbf{y})^T\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "The average gradient is:\n",
    "$$\n",
    "\\nabla L(\\mathbf{w})^T=\\frac{1}{m}(\\mathbf{y}_{pred}-\\mathbf{y})^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "Update the weights:\n",
    "$$\n",
    "\\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\nabla L(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Repeat the above steps for 200 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5333758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" add a column of ones to the matrix\n",
    "    \"\"\"\n",
    "    X0 = np.ones((X.shape[0], 1))\n",
    "    return np.hstack((X0, X))\n",
    "\n",
    "\n",
    "class MyPerceptron:\n",
    "    def __init__(self, lr: float) -> None:\n",
    "        self.lr: float = lr  # learning rate\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, seed: int = 0) -> None:\n",
    "        assert X.ndim == 2\n",
    "        assert y.ndim == 1\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X = add_ones_column(X)\n",
    "        m = X.shape[0]  # number of samples\n",
    "        n = X.shape[1]  # number of features\n",
    "\n",
    "        # weights\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        init_W = rng.random(n)\n",
    "        init_W /= np.linalg.norm(init_W)  # initial weights are a random unit vector\n",
    "        self.W: np.ndarray = init_W\n",
    "\n",
    "        for epoch in range(200):\n",
    "            # predictions for all samples. vector of size m\n",
    "            y_pred = np.where(np.dot(X, self.W) > 0, 1, 0)\n",
    "\n",
    "            # (average) gradient for all samples. vector of size n\n",
    "            gradient = (y_pred - y) @ X / m\n",
    "\n",
    "            # loss for all samples\n",
    "            loss = (y_pred - y) @ (X @ self.W)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= self.lr * gradient\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = add_ones_column(X)\n",
    "        return np.where(np.dot(X, self.W) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21657f5d",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31b772",
   "metadata": {},
   "source": [
    "To reduce the randomness of train_test_split, I split the dataset with 50 different random_states. For each split I train the off-the-shelf implementation and mine using the training set and predict the labels for the test set. I concatenate the $y_{actual}$ and $y_{pred}$ of each split and use them to compute the metrics.\n",
    "\n",
    "My implementation performs better than off-the-shelf on f1 score. This may result from batch gradient descent, whereas the off-the-shelf uses stochastic gradient descent.\n",
    "\n",
    "I also compute the cosine similarity between the weights of off-the-shelf perceptron and mine. The result shows that the weights of my implementation are very similar to that of off-the-shelf. It is much higher than the average similarity of two random 15-dimensional vectors (around 0.756). This demonstrates that my implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4afb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    \"\"\" cosine similarity\n",
    "    \"\"\"\n",
    "    return x @ y / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "\n",
    "def run_perceptron(X: pd.DataFrame, y: pd.Series):\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X = preprocessing_perceptron(X)\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    y_test_all = np.empty(shape=(0,))\n",
    "    y_pred_shelf_all = np.empty(shape=(0,))\n",
    "    y_pred_my_all = np.empty(shape=(0,))\n",
    "    cosine_all = np.empty(shape=(0,))\n",
    "    for r in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.1, random_state=r, stratify=y)\n",
    "        y_test_all = np.append(y_test_all, y_test)\n",
    "\n",
    "        # off-the-shelf library\n",
    "        shelf = Perceptron(tol=1e-3, random_state=r)\n",
    "        shelf.fit(X_train, y_train)  # train using training set\n",
    "        y_pred_shelf = shelf.predict(X_test)  # predict for test set\n",
    "        y_pred_shelf_all = np.append(y_pred_shelf_all, y_pred_shelf)\n",
    "        W_shelf = np.append(shelf.intercept_, shelf.coef_.flatten())  # weights of library implementation\n",
    "\n",
    "        # my implementation\n",
    "        my = MyPerceptron(1)\n",
    "        my.fit(X_train, y_train, seed=r)  # train using training set\n",
    "        y_pred_my = my.predict(X_test)  # predict for test set\n",
    "        y_pred_my_all = np.append(y_pred_my_all, y_pred_my)\n",
    "        W_my = my.W  # weights of my implementation\n",
    "\n",
    "        # cosine similarity\n",
    "        cosine = cosine_sim(W_shelf, W_my)\n",
    "        cosine_all = np.append(cosine_all, cosine)\n",
    "\n",
    "    print('[+] off-the-shelf implementation:')\n",
    "    evaluate(y_test_all, y_pred_shelf_all)\n",
    "    print()\n",
    "\n",
    "    print('[+] my implementation:')\n",
    "    evaluate(y_test_all, y_pred_my_all)\n",
    "    print()\n",
    "\n",
    "    # average cosine similarity\n",
    "    # between the weights of library and my implementation\n",
    "    print(\"[+] average cosine similarity:\", cosine_all.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333bc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] off-the-shelf implementation:\n",
      "accuracy: 0.6788679245283019\n",
      "precision: 0.6497797356828194\n",
      "recall: 0.6622448979591836\n",
      "f1: 0.6559531028906408\n",
      "confusion matrix:\n",
      "[[3951 1749]\n",
      " [1655 3245]]\n",
      "\n",
      "[+] my implementation:\n",
      "accuracy: 0.6800943396226415\n",
      "precision: 0.5956880152187698\n",
      "recall: 0.9585714285714285\n",
      "f1: 0.7347673054360578\n",
      "confusion matrix:\n",
      "[[2512 3188]\n",
      " [ 203 4697]]\n",
      "\n",
      "[+] average cosine similarity: 0.9453590450484168\n"
     ]
    }
   ],
   "source": [
    "run_perceptron(X.copy(), y.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
