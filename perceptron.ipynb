{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9678a1ad",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c372f",
   "metadata": {},
   "source": [
    "## Student Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa4297",
   "metadata": {},
   "source": [
    "|Name|NetId|\n",
    "|-|-|\n",
    "|Zhengchuan Liang|zlian064|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d3db9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844ba18",
   "metadata": {},
   "source": [
    "Some libraries should be imported first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9179238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22957553",
   "metadata": {},
   "source": [
    "Load our dataset into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6580524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f9c95",
   "metadata": {},
   "source": [
    "Get `X` (a matrix of the features values) and `y` (a vector of the labels) from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b29f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "y: pd.Series = df['NObeyesdad'].str.startswith('Obesity').astype(int)\n",
    "del df['NObeyesdad']\n",
    "del df['Height']\n",
    "del df['Weight']\n",
    "X: pd.DataFrame = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a673e17",
   "metadata": {},
   "source": [
    "Accuracy, precision, recall and f1 are our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730be25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared code\n",
    "def evaluate(y_actual, y_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    print('accuracy:', accuracy_score(y_actual, y_pred))\n",
    "    print('precision:', precision_score(y_actual, y_pred))\n",
    "    print('recall:', recall_score(y_actual, y_pred))\n",
    "    print('f1:', f1_score(y_actual, y_pred))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c3fca",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bfad9",
   "metadata": {},
   "source": [
    "Since perceptron only accepts numeric values, I need to map categorical values to numeric ones.\n",
    "For `MTRANS` feature, I believe the number reflects the amount of exercise.\n",
    "\n",
    "Then normalize all features using min-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af42f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_perceptron(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    freq_map = {'no': 0, 'Sometimes': 1,\n",
    "                'Frequently': 2, 'Always': 3}\n",
    "\n",
    "    yes_no_map = {'yes': 1, 'no': 0}\n",
    "\n",
    "    X = X.copy()\n",
    "    X['Gender'] = X['Gender'].map({'Male': 1, 'Female': 0})\n",
    "    X['family_history_with_overweight'] = X['family_history_with_overweight'].map(\n",
    "        yes_no_map)\n",
    "    X['FAVC'] = X['FAVC'].map(yes_no_map)\n",
    "    X['CAEC'] = X['CAEC'].map(freq_map)\n",
    "    X['SMOKE'] = X['SMOKE'].map(yes_no_map)\n",
    "    X['SCC'] = X['SCC'].map(yes_no_map)\n",
    "    X['CALC'] = X['CALC'].map(freq_map)\n",
    "    X['MTRANS'] = X['MTRANS'].map(\n",
    "        {'Automobile': 0, 'Motorbike': 0, 'Public_Transportation': 0, 'Walking': 1, 'Bike': 1})\n",
    "\n",
    "    X = (X-X.min())/(X.max()-X.min())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafe971",
   "metadata": {},
   "source": [
    "## My Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bc601",
   "metadata": {},
   "source": [
    "The activation function of my perceptron is a step funcion:\n",
    "\n",
    "\\begin{equation}\n",
    "F(net)=\n",
    "\\begin{cases}\n",
    "1 & net>0 \\\\\n",
    "0 & net\\le 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$$\n",
    "net=\\sum_{i=0}^nw_ix_i=\\mathbf{w}^T\\mathbf{x}\\text{, where }x_0=1\n",
    "$$\n",
    "\n",
    "Therefore, the perceptron is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x})=F(\\mathbf{w}^T\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Then we want to minimize the sum of distance between each misclassified sample to the decision boundary.\n",
    "\n",
    "For a single sample $(\\mathbf{x},y)$ where $\\mathbf{x}\\in\\mathbb{R}^n$ and $y\\in\\mathbb{R}$,\n",
    "the distance is:\n",
    "\n",
    "$$\n",
    "d\n",
    "=\\frac{|\\sum_{i=0}^nw_ix_i|}{\\sqrt{\\sum_{i=1}^nw_i^2}}\n",
    "=\\frac{(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}}{||\\mathbf{w}||}\n",
    "$$\n",
    "\n",
    "We can ignore $||\\mathbf{w}||$ (can be proved), then the loss function is:\n",
    "$$\n",
    "L(\\mathbf{w})=(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}\n",
    "$$\n",
    "\n",
    "And the gradient is:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i}L(\\mathbf{w})=(P(\\mathbf{x})-y)x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla L(\\mathbf{w})=(P(\\mathbf{x})-y)\\mathbf{x}\n",
    "$$\n",
    "\n",
    "In my implementation, I use **batch gradient** for all samples instead.\n",
    "\n",
    "Given features matrix $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$ and label vector $\\mathbf{y}\\in\\mathbb{R}^{m}$, the steps to train a perceptron are as follows:\n",
    "\n",
    "Initialize weights $\\mathbf{w}$ with a random vector.\n",
    "\n",
    "Predict the labels $\\mathbf{y}_{pred}$ using the perceptron.\n",
    "\n",
    "The sum of loss is:\n",
    "$$\n",
    "L(\\mathbf{w})\n",
    "=\\sum_{(\\mathbf{x},y)}(P(\\mathbf{x})-y)\\mathbf{w}^T\\mathbf{x}\n",
    "=(\\mathbf{y}_{pred}-\\mathbf{y})^T\\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "The average gradient is:\n",
    "$$\n",
    "\\nabla L(\\mathbf{w})^T=\\frac{1}{m}(\\mathbf{y}_{pred}-\\mathbf{y})^T\\mathbf{X}\n",
    "$$\n",
    "\n",
    "Update the weights:\n",
    "$$\n",
    "\\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\nabla L(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Repeat the above steps for 200 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5333758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" add a column of ones to the matrix\n",
    "    \"\"\"\n",
    "    X0 = np.ones((X.shape[0], 1))\n",
    "    return np.hstack((X0, X))\n",
    "\n",
    "\n",
    "class MyPerceptron:\n",
    "    def __init__(self, lr: float) -> None:\n",
    "        self.lr: float = lr  # learning rate\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, seed: int = 0) -> None:\n",
    "        assert X.ndim == 2\n",
    "        assert y.ndim == 1\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X = add_ones_column(X)\n",
    "        m = X.shape[0]  # number of samples\n",
    "        n = X.shape[1]  # number of features\n",
    "\n",
    "        # weights\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        init_W = rng.random(n)\n",
    "        init_W /= np.linalg.norm(init_W)  # initial weights are a random unit vector\n",
    "        self.W: np.ndarray = init_W\n",
    "\n",
    "        for epoch in range(200):\n",
    "            # predictions for all samples. vector of size m\n",
    "            y_pred = np.where(np.dot(X, self.W) > 0, 1, 0)\n",
    "\n",
    "            # (average) gradient for all samples. vector of size n\n",
    "            gradient = (y_pred - y) @ X / m\n",
    "\n",
    "            # loss for all samples\n",
    "            loss = (y_pred - y) @ (X @ self.W)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= self.lr * gradient\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = add_ones_column(X)\n",
    "        return np.where(np.dot(X, self.W) > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21657f5d",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31b772",
   "metadata": {},
   "source": [
    "To reduce the randomness of train_test_split, I split the dataset with 50 different random_states. For each split I train the off-the-shelf implementation and mine using the training set and predict the labels for the test set. I concatenate the $y_{actual}$ and $y_{pred}$ of each split and use them to compute the metrics.\n",
    "\n",
    "My implementation performs better than off-the-shelf on f1 score. This may result from batch gradient descent, whereas the off-the-shelf uses stochastic gradient descent.\n",
    "\n",
    "I also compute the cosine similarity between the weights of off-the-shelf perceptron and mine. The result shows that the weights of my implementation are very similar to that of off-the-shelf. It is much higher than the average similarity of two random 15-dimensional vectors (around 0.756). This demonstrates that my implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4afb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    return x @ y / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "\n",
    "def run_perceptron(X: pd.DataFrame, y: pd.Series):\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, pair_confusion_matrix\n",
    "\n",
    "    X = preprocessing_perceptron(X)\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    accuracy_my_all = np.empty(shape=(0,))\n",
    "    accuracy_shelf_all = np.empty(shape=(0,))\n",
    "\n",
    "    precision_my_all = np.empty(shape=(0,))\n",
    "    precision_shelf_all = np.empty(shape=(0,))\n",
    "\n",
    "    recall_my_all = np.empty(shape=(0,))\n",
    "    recall_shelf_all = np.empty(shape=(0,))\n",
    "\n",
    "    f1_my_all = np.empty(shape=(0,))\n",
    "    f1_shelf_all = np.empty(shape=(0,))\n",
    "\n",
    "    cosine_all = np.empty(shape=(0,))\n",
    "\n",
    "    skf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # off-the-shelf library\n",
    "        shelf = Perceptron(eta0=1.0)\n",
    "        shelf.fit(X_train, y_train)  # train using training set\n",
    "        y_pred_shelf = shelf.predict(X_test)  # predict for test set\n",
    "        W_shelf = np.append(shelf.intercept_, shelf.coef_.flatten())  # weights of library implementation\n",
    "        accuracy_shelf_all = np.append(accuracy_shelf_all, accuracy_score(y_test, y_pred_shelf))\n",
    "        precision_shelf_all = np.append(precision_shelf_all, precision_score(y_test, y_pred_shelf, zero_division=0))\n",
    "        recall_shelf_all = np.append(recall_shelf_all, recall_score(y_test, y_pred_shelf, zero_division=0))\n",
    "        f1_shelf_all = np.append(f1_shelf_all, f1_score(y_test, y_pred_shelf, zero_division=0))\n",
    "\n",
    "        # my implementation\n",
    "        my = MyPerceptron(lr=1)\n",
    "        my.fit(X_train, y_train)  # train using training set\n",
    "        y_pred_my = my.predict(X_test)  # predict for test set\n",
    "        W_my = my.W  # weights of my implementation\n",
    "        accuracy_my_all = np.append(accuracy_my_all, accuracy_score(y_test, y_pred_my))\n",
    "        precision_my_all = np.append(precision_my_all, precision_score(y_test, y_pred_my, zero_division=0))\n",
    "        recall_my_all = np.append(recall_my_all, recall_score(y_test, y_pred_my, zero_division=0))\n",
    "        f1_my_all = np.append(f1_my_all, f1_score(y_test, y_pred_my, zero_division=0))\n",
    "\n",
    "        # cosine similarity\n",
    "        cosine = cosine_sim(W_shelf, W_my)\n",
    "        cosine_all = np.append(cosine_all, cosine)\n",
    "\n",
    "    print('[+] Accuracy')\n",
    "    print('[shelf] mean {:.3f}; std {:.3f}'.format(accuracy_shelf_all.mean(), accuracy_shelf_all.std()))\n",
    "    print('[mine]  mean {:.3f}; std {:.3f}'.format(accuracy_my_all.mean(), accuracy_my_all.std()))\n",
    "    print()\n",
    "\n",
    "    print('[+] Precision')\n",
    "    print('[shelf] mean {:.3f}; std {:.3f}'.format(precision_shelf_all.mean(), precision_shelf_all.std()))\n",
    "    print('[mine]  mean {:.3f}; std {:.3f}'.format(precision_my_all.mean(), precision_my_all.std()))\n",
    "    print()\n",
    "\n",
    "    print('[+] Recall')\n",
    "    print('[shelf] mean {:.3f}; std {:.3f}'.format(recall_shelf_all.mean(), recall_shelf_all.std()))\n",
    "    print('[mine]  mean {:.3f}; std {:.3f}'.format(recall_my_all.mean(), recall_my_all.std()))\n",
    "    print()\n",
    "\n",
    "    print('[+] F1')\n",
    "    print('[shelf] mean {:.3f}; std {:.3f}'.format(f1_shelf_all.mean(), f1_shelf_all.std()))\n",
    "    print('[mine]  mean {:.3f}; std {:.3f}'.format(f1_my_all.mean(), f1_my_all.std()))\n",
    "    print()\n",
    "\n",
    "    # average cosine similarity\n",
    "    # between the weights of library and my implementation\n",
    "    print('[+] Average Cosine Similarity {:.3f}'.format(cosine_all.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333bc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Accuracy\n",
      "[shelf] mean 0.685; std 0.068\n",
      "[mine]  mean 0.690; std 0.056\n",
      "\n",
      "[+] Precision\n",
      "[shelf] mean 0.641; std 0.156\n",
      "[mine]  mean 0.606; std 0.049\n",
      "\n",
      "[+] Recall\n",
      "[shelf] mean 0.706; std 0.316\n",
      "[mine]  mean 0.969; std 0.029\n",
      "\n",
      "[+] F1\n",
      "[shelf] mean 0.625; std 0.215\n",
      "[mine]  mean 0.744; std 0.032\n",
      "\n",
      "[+] Average Cosine Similarity 0.950\n"
     ]
    }
   ],
   "source": [
    "run_perceptron(X.copy(), y.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}